{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ulgnkbbvEt6u"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is NOT available!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "cuda_is_available = torch.cuda.is_available()\n",
    "\n",
    "if cuda_is_available:\n",
    "    print(\"All good!\")\n",
    "else:\n",
    "    print(\"CUDA is NOT available!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "7R6RnqQu0-P9"
   },
   "outputs": [],
   "source": [
    "# You can use this prompt or try something else!\n",
    "prompt = \"During the latest presentation OpenAI\"\n",
    "# A good model for this exercise, but feel free to use another model: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads\n",
    "model = \"openai-community/gpt2-large\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "hkVKzDegAItu"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hwind\\anaconda3\\envs\\pytorch_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "# Step 1 - Set Up the Text Generation Pipeline\n",
    "\n",
    "# TODO: Import \"pipeline\" function\n",
    "from transformers import pipeline, set_seed\n",
    "\n",
    "\n",
    "# TODO: Create a pipeline for text generation, selected model, and \"device=0\" (to use CUDA)\n",
    "generator = pipeline('text-generation', model='gpt2-large')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "_xUAq3XiAN1z"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'During the latest presentation OpenAI is running on Intel\\'s newest chips. The demo was for a simple 2-way race, in which the \"AI\" was running against 5 human drivers who were given a set course and given a set number of laps to complete. It turns out that the AI was much more powerful than the other 5.\\n\\nOn the first lap each driver had their set number of laps to complete. After this, they were told that they had until the time limit had expired'}]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 2 - Generate text with the default settings\n",
    "\n",
    "# TODO: Generate text with the selected prompt\n",
    "# Recommended parameters:\n",
    "# max_length=100 - generate up to 100 tokens\n",
    "# num_return_sequences=1 - only return one generated sequence\n",
    "set_seed(42)\n",
    "generator(prompt, max_length=100, num_return_sequences=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "rGcDDPcz1Ln5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'During the latest presentation OpenAI\\'s CEO, John Robb, said that the company is working on a \"biggest AI project ever\" and that it is \"going to be a big deal.\"\\n\\nRobb also said that the company is working on a \"biggest AI project ever\" and that it is \"going to be a big deal.\"\\n\\n\"We\\'re going to be a big deal,\" he said. \"We\\'re going to be the biggest AI project ever.\"\\n\\n'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3 - Experiment with Different Parameters\n",
    "\n",
    "# Now try different parameters\n",
    "\n",
    "# TODO: Try text generation with \"do_sample\" parameter equal to `True` or `False`\n",
    "# generator(prompt, max_length=100, num_return_sequences=1, do_sample=True)\n",
    "# \"During the latest presentation OpenAI has published a paper with a model for what robots see and interpret when they are being simulated. This is an artificial intelligence platform based on the framework provided by Google and a little knowledge of the underlying neural networks can help you improve the performance of your bot.\\n\\nOpenAI is a California-based non profit organization, with the goal of advancing AI and machine learning towards our shared goal of making the human condition better, safer, and more fulfilling.\\n\\nWe're\"\n",
    "\n",
    "generator(prompt, max_length=100, num_return_sequences=1, do_sample=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uo3CC3yk4QmC"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'During the latest presentation OpenAI\\'s CEO, John Robb, said that the company is working on a \"biggest AI project ever\" and that it is \"going to be a big deal.\"\\n\\nRobb also said that the company is working on a \"biggest AI project ever\" and that it is \"going to be a big deal.\"\\n\\n\"We\\'re going to be a big deal,\" he said. \"We\\'re going to be the biggest AI project ever.\"\\n\\n'}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Try text generation using \"Beam-search strategy\"\n",
    "# Parameters:\n",
    "# do_sample=False (greedy search) True (Multinomial sampling)\n",
    "# num_beams - try different values. Beam = 1 is the same as greedy search\n",
    "\n",
    "generator(prompt, max_length=100, num_return_sequences=1, do_sample=False, num_beams=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "K96hVRPC6MUR"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'During the latest presentation OpenAI CEO and co-founder Demis Hassabis gave at the recent Deep Learning Summit in San Francisco, Hassabis was asked about the future of artificial intelligence. He said:\\n\\n\"I don\\'t think we\\'re there yet. I think we\\'re going to get there. I think we\\'re going to get there. I think we\\'re going to get there. I think we\\'re going to get there. I think we\\'re going to get there. I think we'}]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Try text generation using \"Beam-search multinomial sampling\"\n",
    "# Parameters:\n",
    "# do_sample=True\n",
    "# num_beams - try different values\n",
    "generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, num_beams=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "f5R7GeD3AUgp"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'During the latest presentation OpenAI CEO Demis Hassabis told the audience that the team is currently working on a number of different projects. One of these is an AI-powered version of the popular card game Magic: The Gathering.\\n\\n\"We are working on an AI-powered version of Magic: The Gathering,\" said Hassabis. \"We are also working on an AI-powered version of Hearthstone. We are also working on an AI-powered version of Settlers of Catan.\"\\n'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: Try text generation with different \"top_k\" values. E.g. from 1 to 500\n",
    "generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, num_beams=5, top_k=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXOuaxe0RJwf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature: 0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'During the latest presentation OpenAI\\'s CEO and co-founder Demis Hassabis said:\\n\\n\"We\\'ve been working on this for a long time. We\\'ve been working on this for a long time. We\\'ve been working on this for a long time. We\\'ve been working on this for a long time. We\\'ve been working on this for a long time. We\\'ve been working on this for a long time. We\\'ve been working on this for a long time. We\\'ve'}]\n",
      "\n",
      "\n",
      "Temperature: 3.0\n",
      "[{'generated_text': 'During the latest presentation OpenAI\\'s leader, Vicente Iñiguez, had this to say:\\n\\n\"The real difficulty is finding ways to scale this problem, and then how do you ensure it\\'s going to be beneficial to society in a practical sense. So we don\\'t have solutions to that. The key question is how to get humans to help the AI achieve these goals, and we know how to start with our own efforts, which is to give people more opportunities to advance their'}]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TODO: Try text generation with different \"temperature\" values. E.g. from 0.1 to 3.0. High temperature values\n",
    "# will produce more random results, while lower values will produce more deterministic results\n",
    "generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, num_beams=5, temperature=0.7)\n",
    "\n",
    "for temp in [0.1, 3.0]:\n",
    "    print(f\"Temperature: {temp}\")\n",
    "    generated_text = generator(prompt, max_length=100, num_return_sequences=1, do_sample=True, num_beams=5, temperature=temp)\n",
    "    print(generated_text[0]['generated_text'])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUiu5ZdT9ArW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "pytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
